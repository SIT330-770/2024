---
type: lecture
week: Week 10
date: 2024-05-13T10:00:00
title: Transformers and Pretrained LMs
tldr: "Transformers and Pretrained LMs."
hide_from_announcments: true
thumbnail: /static_files/presentations/transformers.jpg
links: 
    - url: /static_files/presentations/Week_10_-_Transformers_and_Pretrained_LMs.pdf
      name: slides
    - url: /static_files/presentations/Week_10_-_Transformers_and_Pretrained_LMs_6up.pdf
      name: slides 6up
---
**Video recordings (X Hour, XX Minutes and XX Seconds):**
- Transformers: Attention Is All You Need! (X Hour, XX Minutes and XX Seconds)
    - [Introduction to Transformers (16:23)](https://youtu.be/KCqihbmWeao)
    - Self-Attention Mechanism
    - The Encoder Transformer Block
    - The Input: Embeddings for Tokens
    - The Input: Embeddings for Positions
    - The Task Specific Head
- Pre-trained LMs (X Hour, XX Minutes and XX Seconds)
    - BERT: Bidirectional Encoder Representations from Transformers
    - BERT pre-training
    - BERT fine-tuning
    - BERT Performance
    - Other Models Based on Transformers
    - HuggingFace
