![image](https://github.com/rbouadjenek/SIT330-770/assets/7735263/f2a59530-90af-43e1-92f0-46535fad8529)![image](https://github.com/rbouadjenek/SIT330-770/assets/7735263/8fa484bf-ad96-4993-8fba-45857b31b6ab)---
type: lecture
week: Week 10
date: 2024-05-13T10:00:00
title: Transformers and Pretrained LMs
tldr: "Transformers and Pretrained LMs."
hide_from_announcments: true
thumbnail: /static_files/presentations/transformers.jpg
links: 
    - url: /static_files/presentations/
      name: slides
    - url: /static_files/presentations/
      name: slides 6up
---
**Video recordings (X Hour, XX Minutes and XX Seconds):**
- Introduction to Transformers
- Self-Attention Mechanism
- The Transformer Block
- The Input: Embeddings for Tokens
- The Input: Embeddings for Positions
- The Task Specific Head
- BERT: Bidirectional Encoder Representations from Transformers
- BERT pre-training
- BERT fine-tuning
